---
title: "我的机器学习入门指南"
date: 2024-03-25T14:30:00+08:00
language: zh-cn
weight: 100
draft: false
categories:
- 技术探讨
tags:
- 机器学习
- 人工智能
keywords:
- 监督学习
- 无监督学习
- 深度学习
---



**▌预处理是什么的？**
现实数据都特脏：有缺失、有噪声、单位还乱七八糟（比如身高1.8m vs 体重80kg）。不处理直接喂给模型，容易死
**工具包必装**：
```bash
pip install scikit-learn  
```



**归一化：
**情况1：最大最小归一化**
- **公式**：（当前值 - 这列最小值）/（这列最大值 - 最小值）
- **适合**：数据分布均匀，比如像素值（0-255）
- **缺点**：来个身高3米的巨人，所有数据都被压成0.5以下

**情况2：标准化**
- **公式**：（当前值 - 平均值）/标准差
- **适合**：大部分情况！特别是数据有偏移（比如收入数据右偏）
- **优点**：不怕异常值，来个月薪100万的大佬也不慌

*代码参考：normalization.ipynb（记得sklearn的MinMaxScaler和StandardScaler）*



降维：实在跑不动再搞
**PCA（主成分分析）**：
- 找数据方差最大的方向，相当于“抓主要矛盾”
- 适合：特征多但样本少（比如基因数据）
- 缺点：只能处理线性关系

**SVD（奇异值分解）**：
- 矩阵拆拆拆，拆出核心成分
- 适合：特征和样本数量差不多（比如用户-商品评分矩阵）
- *玄学点说：PCA是SVD的特例*

*代码参考：dimension_reduction.ipynb（sklearn.decomposition里都有）*

数据集必须分！**
- **训练集**（70%）：给模型当教材
**测试集**（30%）：期末大考，最后才能用
 **过拟合**：模型把教材里的错别字都背下来了，考试直接懵逼

*划分类代码：dataset_split.ipynb（train_test_split函数）*

todo

1. 归一化优先用标准化，除非数据特别规整
2. 降维不是必须的，但数据上百万特征时能救命
3. 不划分测试集≈开卷考试还作弊，模型会装得很聪明
4. sklearn文档多查查，别重复造轮子
